{"cells":[{"cell_type":"code","source":["import os\n","\n","os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n","\n","import keras\n","from keras import layers\n","from keras import ops\n","from keras.preprocessing import image_dataset_from_directory\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"z7XLysHQDg7-","executionInfo":{"status":"ok","timestamp":1723411162080,"user_tz":240,"elapsed":277,"user":{"displayName":"Duncan Volk","userId":"15461499935065026162"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Path to your dataset in Google Drive\n","train_dir = '/content/drive/MyDrive/APS360 Project/Data/train'\n","test_dir = '/content/drive/MyDrive/APS360 Project/Data/test'\n","\n","# Load the training and testing datasets\n","train_dataset = image_dataset_from_directory(\n","    train_dir,\n","    image_size=(32, 32),  # Resizing to the original CIFAR-100 size\n","    batch_size=batch_size,\n","    label_mode='int',  # Assuming integer labels\n","    shuffle=True\n",")\n","\n","test_dataset = image_dataset_from_directory(\n","    test_dir,\n","    image_size=(32, 32),  # Resizing to the original CIFAR-100 size\n","    batch_size=batch_size,\n","    label_mode='int',  # Assuming integer labels\n","    shuffle=False\n",")\n","\n","# Optionally, you can also create a validation split\n","#val_dataset = train_dataset.take(5000 // batch_size)  # Assuming 10% validation\n","#train_dataset = train_dataset.skip(5000 // batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbG0fakCDqXR","executionInfo":{"status":"ok","timestamp":1723411178780,"user_tz":240,"elapsed":14619,"user":{"displayName":"Duncan Volk","userId":"15461499935065026162"}},"outputId":"1dde0a71-7d13-468c-ea36-1187dd373909"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 28710 files belonging to 7 classes.\n","Found 7178 files belonging to 7 classes.\n"]}]},{"cell_type":"code","source":["learning_rate = 0.001\n","weight_decay = 0.0001\n","batch_size = 256\n","num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n","image_size = 72  # We'll resize input images to this size\n","patch_size = 6  # Size of the patches to be extract from the input images\n","num_patches = (image_size // patch_size) ** 2\n","projection_dim = 64\n","num_heads = 4\n","transformer_units = [\n","    projection_dim * 2,\n","    projection_dim,\n","]  # Size of the transformer layers\n","transformer_layers = 8\n","mlp_head_units = [\n","    2048,\n","    1024,\n","]  # Size of the dense layers of the final classifier"],"metadata":{"id":"jldYLwmDD0hr","executionInfo":{"status":"ok","timestamp":1723411181440,"user_tz":240,"elapsed":203,"user":{"displayName":"Duncan Volk","userId":"15461499935065026162"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.Resizing(image_size, image_size),  # Resize to the new image size\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(factor=0.02),\n","        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n","    ],\n","    name=\"data_augmentation\",\n",")"],"metadata":{"id":"uyN7SNHjD3s2","executionInfo":{"status":"ok","timestamp":1723411184739,"user_tz":240,"elapsed":197,"user":{"displayName":"Duncan Volk","userId":"15461499935065026162"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x"],"metadata":{"id":"JYsPupY-D9ee","executionInfo":{"status":"ok","timestamp":1723411187250,"user_tz":240,"elapsed":239,"user":{"displayName":"Duncan Volk","userId":"15461499935065026162"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super().__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        input_shape = ops.shape(images)\n","        batch_size = input_shape[0]\n","        height = input_shape[1]\n","        width = input_shape[2]\n","        channels = input_shape[3]\n","        num_patches_h = height // self.patch_size\n","        num_patches_w = width // self.patch_size\n","        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n","        patches = ops.reshape(\n","            patches,\n","            (\n","                batch_size,\n","                num_patches_h * num_patches_w,\n","                self.patch_size * self.patch_size * channels,\n","            ),\n","        )\n","        return patches\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\"patch_size\": self.patch_size})\n","        return config"],"metadata":{"id":"H2XMwOVyEAmN","executionInfo":{"status":"ok","timestamp":1723411188728,"user_tz":240,"elapsed":217,"user":{"displayName":"Duncan Volk","userId":"15461499935065026162"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(4, 4))\n","image = train_dataset[np.random.choice(range(train_dataset.shape[0]))]\n","plt.imshow(image.astype(\"uint8\"))\n","plt.axis(\"off\")\n","\n","resized_image = ops.image.resize(\n","    ops.convert_to_tensor([image]), size=(image_size, image_size)\n",")\n","patches = Patches(patch_size)(resized_image)\n","print(f\"Image size: {image_size} X {image_size}\")\n","print(f\"Patch size: {patch_size} X {patch_size}\")\n","print(f\"Patches per image: {patches.shape[1]}\")\n","print(f\"Elements per patch: {patches.shape[-1]}\")\n","\n","n = int(np.sqrt(patches.shape[1]))\n","plt.figure(figsize=(4, 4))\n","for i, patch in enumerate(patches[0]):\n","    ax = plt.subplot(n, n, i + 1)\n","    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n","    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n","    plt.axis(\"off\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"aQUatjAEEDxi","executionInfo":{"status":"error","timestamp":1723411220905,"user_tz":240,"elapsed":223,"user":{"displayName":"Duncan Volk","userId":"15461499935065026162"}},"outputId":"600a0281-e938-4769-8589-95a1de7b42ab"},"execution_count":27,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'_PrefetchDataset' object has no attribute 'shape'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-aa2eb53129b3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: '_PrefetchDataset' object has no attribute 'shape'"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 400x400 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"code","source":["class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super().__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    def call(self, patch):\n","        positions = ops.expand_dims(\n","            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n","        )\n","        projected_patches = self.projection(patch)\n","        encoded = projected_patches + self.position_embedding(positions)\n","        return encoded\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\"num_patches\": self.num_patches})\n","        return config"],"metadata":{"id":"JFIAvJKFEK8L","executionInfo":{"status":"ok","timestamp":1723411251851,"user_tz":240,"elapsed":226,"user":{"displayName":"Duncan Volk","userId":"15461499935065026162"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def run_experiment(model):\n","    optimizer = keras.optimizers.AdamW(\n","        learning_rate=learning_rate, weight_decay=weight_decay\n","    )\n","\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=[\n","            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n","            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n","        ],\n","    )\n","\n","    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_accuracy\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","\n","    history = model.fit(\n","        x=x_train,\n","        y=y_train,\n","        batch_size=batch_size,\n","        epochs=num_epochs,\n","        validation_split=0.1,\n","        callbacks=[checkpoint_callback],\n","    )\n","\n","    model.load_weights(checkpoint_filepath)\n","    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n","\n","    return history\n","\n","\n","vit_classifier = create_vit_classifier()\n","history = run_experiment(vit_classifier)\n","\n","\n","def plot_history(item):\n","    plt.plot(history.history[item], label=item)\n","    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(item)\n","    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","\n","plot_history(\"loss\")\n","plot_history(\"top-5-accuracy\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdc7QfABEUUx","outputId":"8cd23450-9272-4d6f-dffd-62e7a68948a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3217s\u001b[0m 18s/step - accuracy: 0.0185 - loss: 5.3402 - top-5-accuracy: 0.0787 - val_accuracy: 0.0406 - val_loss: 4.3767 - val_top-5-accuracy: 0.1642\n","Epoch 2/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3170s\u001b[0m 18s/step - accuracy: 0.0336 - loss: 4.3873 - top-5-accuracy: 0.1377 - val_accuracy: 0.0642 - val_loss: 4.1696 - val_top-5-accuracy: 0.2258\n","Epoch 3/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3117s\u001b[0m 18s/step - accuracy: 0.0458 - loss: 4.2378 - top-5-accuracy: 0.1868 - val_accuracy: 0.0766 - val_loss: 4.0795 - val_top-5-accuracy: 0.2554\n","Epoch 4/10\n","\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3093s\u001b[0m 17s/step - accuracy: 0.0582 - loss: 4.1420 - top-5-accuracy: 0.2156 - val_accuracy: 0.0870 - val_loss: 3.9892 - val_top-5-accuracy: 0.2884\n","Epoch 5/10\n","\u001b[1m 44/176\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36:53\u001b[0m 17s/step - accuracy: 0.0748 - loss: 4.0326 - top-5-accuracy: 0.2590"]}]},{"cell_type":"code","source":["def create_vit_classifier():\n","    inputs = keras.Input(shape=input_shape)\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","    # Create patches.\n","    patches = Patches(patch_size)(augmented)\n","    # Encode patches.\n","    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(0.5)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n","    # Classify outputs.\n","    logits = layers.Dense(num_classes)(features)\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","    return model"],"metadata":{"id":"wfbOri2-EQSJ","executionInfo":{"status":"ok","timestamp":1723411254791,"user_tz":240,"elapsed":316,"user":{"displayName":"Duncan Volk","userId":"15461499935065026162"}}},"execution_count":29,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYLDqjHFuz4IVIJ/9g7wlX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}