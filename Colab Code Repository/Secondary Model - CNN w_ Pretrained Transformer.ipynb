{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":492},"executionInfo":{"elapsed":16029,"status":"ok","timestamp":1724363283531,"user":{"displayName":"Duncan Volk","userId":"08057092679433673993"},"user_tz":240},"id":"gi562Lcrw8BL","outputId":"dc6b64e2-8c3e-477e-a5c2-86d342355e65"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7795cdb2ccdc4071880181a2898583a0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0576b90cdc44bef8e5e7f1f02650a3f","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["\n","from zipfile import ZipFile\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Subset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","from torchvision.transforms import Resize\n","import matplotlib.pyplot as plt\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data import Subset\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import shutil\n","from torch.utils.data import random_split\n","\n","%pip install transformers\n","\n","from transformers import ViTModel, ViTFeatureExtractor\n","import torch.nn as nn\n","\n","# Load a pretrained ViT model\n","vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","#set use_cuda to true for GPU use\n","use_cuda = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3743,"status":"ok","timestamp":1724363291888,"user":{"displayName":"Duncan Volk","userId":"08057092679433673993"},"user_tz":240},"id":"gSZnLmFatL89","outputId":"a1735243-e8ba-4538-e46f-a2d79c63d050"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting split-folders\n","  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n","Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n","Installing collected packages: split-folders\n","Successfully installed split-folders-0.5.1\n"]}],"source":["%pip install split-folders\n","import splitfolders"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57706,"status":"ok","timestamp":1724363352211,"user":{"displayName":"Duncan Volk","userId":"08057092679433673993"},"user_tz":240},"id":"WkV4gMdNxIuD","outputId":"a82abeac-f307-43a9-928e-6f7492b42ebb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Lfpia3t4lvi"},"outputs":[],"source":["#This ended up not being used\n","'''def folder_combiner(folder_path, destination_path):\n","  for emotion_folder in os.listdir(folder_path):\n","    emotion_folder_path = os.path.join(folder_path, emotion_folder)\n","    if os.path.isdir(emotion_folder_path):\n","        # Iterate through images in each emotion folder\n","        for filename in os.listdir(emotion_folder_path):\n","            if filename.endswith('.jpg') or filename.endswith('.png'):  # Add more extensions if needed\n","                src_path = os.path.join(emotion_folder_path, filename)\n","                dst_path = os.path.join(destination_path, f\"{emotion_folder}_{filename}\")  # Prefix with emotion label\n","                shutil.copy(src_path, dst_path)'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H6SdIR3rpOVh"},"outputs":[],"source":["train_folder_path = '/content/gdrive/My Drive/APS360 Project/kaggle_facial_emotion_data/train'\n","test_folder_path = '/content/gdrive/My Drive/APS360 Project/test'\n","test_destination_path = '/content/gdrive/My Drive/APS360 Project/Combined Data'\n","train_destination_path = ''\n","\n","#folder_combiner(test_folder_path, test_destination_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vksX6UUaz8yi"},"outputs":[],"source":["#Function to combine all of the class folders into a single folder\n","#Was supposed to do same thing as above function, also not used.\n","'''def load_images(folder_path):\n","    images = []\n","    labels = []\n","    emotion_folders = os.listdir(folder_path)\n","    for emotion_folder in emotion_folders:\n","        emotion_path = os.path.join(folder_path, emotion_folder)\n","        if os.path.isdir(emotion_path):\n","            label = emotion_folder  # Use folder name as the label\n","            for filename in os.listdir(emotion_path):\n","                img_path = os.path.join(emotion_path, filename)\n","                if img_path.endswith(\".jpg\") or img_path.endswith(\".png\"):  # Add more extensions if needed\n","                    img = cv2.imread(img_path)\n","                    if img is not None:\n","                        images.append(img)\n","                        labels.append(label)\n","    return images, labels'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hi7AbsQVtgBp"},"outputs":[],"source":["splitfolders.ratio('/content/gdrive/MyDrive/APS360 Project/Data/train', output=\"split_data\",\n","    seed=999, ratio=(0.8, 0.1, 0.1), group_prefix=None, move=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RnWHppBfva_w"},"outputs":[],"source":["transform = transforms.Compose(\n","        [transforms.ToTensor(),transforms.Resize((224,224))])\n","\n","#transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224,224))])\n","\n","#Create train, validation, and testing datasets\n","#Apply the transformations\n","\n","train_data = ImageFolder(\"/content/gdrive/MyDrive/APS360 Project/Data/train\", transform)\n","\n","train_size = int(0.8 * len(train_data))\n","val_size = len(train_data) - train_size\n","\n","train_data, val_data = random_split(train_data, [train_size, val_size])\n","test_data = ImageFolder(\"/content/gdrive/MyDrive/APS360 Project/Data/test\", transform)\n","\n","#Load all of the datasets into their respective loaders\n","#train_loader = torch.utils.data.DataLoader(train_data, batch_size=10,\n","                                           #shuffle=True)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True, num_workers=2)\n","val_loader = torch.utils.data.DataLoader(val_data, batch_size=10,\n","                                         shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=10,\n","                                          shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOMw49C8wfS8"},"outputs":[],"source":["class HybridEmotionClassifier(nn.Module):\n","    def __init__(self, name=\"HybridEmotionClassifier\"):\n","        super(HybridEmotionClassifier, self).__init__()\n","         # CNN components\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)\n","        self.conv2 = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        # Fully connected layers\n","        self.fc1_cnn = nn.Linear(10 * 54 * 54, 128)  # 29160 -> 128\n","        self.fc2_cnn = nn.Linear(128, 64)            # 128 -> 64\n","\n","        # ViT components\n","        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","        self.fc1_vit = nn.Linear(768, 32)  # 768 is the output dimension of ViT\n","\n","        # Combined fully connected layer\n","        self.fc_combined = nn.Linear(96, 7)  # Adjusted from 64 to 96\n","\n","    def forward(self, x):\n","        # CNN forward pass\n","        x_cnn = self.pool(F.relu(self.conv1(x)))\n","        x_cnn = self.pool(F.relu(self.conv2(x_cnn)))\n","        x_cnn = x_cnn.view(x_cnn.size(0), -1)  # Flatten the output for the fully connected layer\n","        x_cnn = F.relu(self.fc1_cnn(x_cnn))\n","        x_cnn = F.relu(self.fc2_cnn(x_cnn))\n","\n","        # ViT forward pass\n","        vit_outputs = self.vit(pixel_values=x)['last_hidden_state']\n","        x_vit = vit_outputs[:, 0]  # [CLS] token output\n","        x_vit = F.relu(self.fc1_vit(x_vit))\n","\n","        # Concatenate CNN and ViT features\n","        x_combined = torch.cat((x_cnn, x_vit), dim=1)\n","\n","        # Final classification layer\n","        x_out = self.fc_combined(x_combined)\n","\n","        return x_out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zO4OChzywuCN"},"outputs":[],"source":["#Get model name for checkpointing\n","def get_model_name(name, batch_size, learning_rate, epoch):\n","   path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size,\n","                                                  learning_rate, epoch)\n","   return path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bm7i5jBvwu6N"},"outputs":[],"source":["#Find the accuracy of the model\n","def get_accuracy(model, data_loader):\n","    #initialize counters\n","    correct = 0\n","    total = 0\n","    for imgs, labels in data_loader:\n","        #Enable GPU if available\n","        if use_cuda and torch.cuda.is_available():\n","          imgs = imgs.cuda()\n","          labels = labels.cuda()\n","\n","        output = model(imgs)\n","        #select index with maximum prediction score\n","        pred = output.max(1, keepdim=True)[1]\n","        correct += pred.eq(labels.view_as(pred)).sum().item()\n","        total += imgs.shape[0]\n","    #Return accuracy\n","    return correct / total"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pO7lZwoCwxVJ"},"outputs":[],"source":["#plot accuracy\n","def plot_training_curve(path, num_epochs, train_acc, val_acc):\n","    plt.title(\"Training Curve\")\n","    n = len(num_epochs) # number of epochs\n","    plt.plot(range(1,n+1), train_acc, label=\"Train\")\n","    plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NT6armZDwzsM"},"outputs":[],"source":["def train(model, train_loader, val_loader, batch_size=64, num_epochs=15,\n","          learn_rate=0.001, model_name='HybridEmotionClassifier'):\n","    #Set seed\n","    torch.manual_seed(1700)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n","\n","    #populate accuracy arrays with zeroes\n","    train_acc = np.zeros(num_epochs)\n","    val_acc = np.zeros(num_epochs)\n","\n","    #initialize iteration counter\n","    n = 0\n","    for epoch in range(num_epochs):\n","        for images, labels in iter(train_loader):\n","            #Enable GPU if available\n","            if use_cuda and torch.cuda.is_available():\n","              images = images.cuda()\n","              labels = labels.cuda()\n","\n","            #perform forwards pass\n","            out = model(images)\n","            #Compute loss using Cross Entropy loss function\n","            loss = criterion(out, labels)\n","            #backward pass to calculate and update parameters.\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            #increment iteration counter\n","            n += 1\n","\n","        # track training and validation accuracy\n","        train_acc[epoch] = get_accuracy(model, train_loader)\n","        val_acc[epoch] = get_accuracy(model, val_loader)\n","\n","        #output accuracy\n","        print((\"Epoch: {} | Training acc: {} |\" + \"Validation acc: {}\"\n","        ).format(epoch, train_acc[epoch], val_acc[epoch]))\n","\n","        #checkpoint model\n","        model_path = get_model_name(model_name, batch_size, learn_rate,\n","                                    epoch)\n","        torch.save(model.state_dict(), model_path)\n","\n","    epochs = np.arange(1, num_epochs + 1)\n","\n","    #plot training curve\n","    plot_training_curve(model_path, epochs, train_acc, val_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Uo19GYww8U5"},"outputs":[],"source":["#Overfitting on small dataset for sanity check\n","\n","model = HybridEmotionClassifier()\n","\n","#sample 10 random indices from the training data\n","indices = np.random.choice(len(train_data), size=10, replace=False)\n","#create an overfit dataset as a subset of the training dataset\n","overfit_data = Subset(train_data, indices)\n","#Load the overfit dataset\n","overfit_loader = DataLoader(overfit_data, batch_size=2, shuffle=True)\n","\n","#Enable GPU if available.\n","if use_cuda and torch.cuda.is_available():\n","  model.cuda()\n","\n","train(model, overfit_loader, overfit_loader, num_epochs=30, batch_size=27,\n","      learn_rate=0.001)\n","#Please note that since I passed the overfit_loader for both the\n","#parameters train_loader and val_loader, the training and validation\n","#accuracy seen here will be identical because they are the same thing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vG56Oi4gG0y9"},"outputs":[],"source":["model1 = HybridEmotionClassifier()\n","if use_cuda and torch.cuda.is_available():\n","  model1.cuda()\n","\n","train(model1, train_loader, val_loader, num_epochs = 30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8QN9hcxrLfl5"},"outputs":[],"source":["path = get_model_name(model1.name, batch_size=64, learning_rate=0.001,\n","                      epoch=11)\n","state = torch.load(path)\n","model1.load_state_dict(state)\n","\n","criterion = nn.CrossEntropyLoss()\n","test_acc = get_accuracy(model1, test_loader)\n","print(test_acc)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1_31NT4_RKnAhRDXq4snFYvIK_J5-Cw35","timestamp":1723479339699}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}