{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8766,"status":"ok","timestamp":1723384803599,"user":{"displayName":"Steven Tian","userId":"04559681777298250097"},"user_tz":240},"id":"H6Jzh_i17km1"},"outputs":[],"source":["from zipfile import ZipFile\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data import Subset\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import shutil\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling1D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","\n","from tensorflow.keras import mixed_precision\n","from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Add"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24512,"status":"ok","timestamp":1723384828109,"user":{"displayName":"Steven Tian","userId":"04559681777298250097"},"user_tz":240},"id":"Pxs-P0PG9f6i","outputId":"20fd7757-7fe5-434c-827b-5f6911c07ecc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723384828109,"user":{"displayName":"Steven Tian","userId":"04559681777298250097"},"user_tz":240},"id":"PdoEWrWA9zZ2"},"outputs":[],"source":["train_dir = '/content/drive/MyDrive/APS360 Project/Data/train'\n","test_dir = '/content/drive/MyDrive/APS360 Project/Data/test'"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61061,"status":"ok","timestamp":1723384889168,"user":{"displayName":"Steven Tian","userId":"04559681777298250097"},"user_tz":240},"id":"IdMNjm_s-C8b","outputId":"e0d8162a-c188-4c37-90dc-caba9b7f82a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 28710 files belonging to 7 classes.\n","Using 22968 files for training.\n","Found 28710 files belonging to 7 classes.\n","Using 5742 files for validation.\n","Found 7178 files belonging to 7 classes.\n"]}],"source":["batch_size = 32\n","img_size = (224, 224)\n","\n","train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","    train_dir,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=123,\n","    image_size=img_size,\n","    batch_size=batch_size\n",")\n","\n","val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","    train_dir,\n","    validation_split=0.2,\n","    subset=\"validation\",\n","    seed=123,\n","    image_size=img_size,\n","    batch_size=batch_size\n",")\n","\n","test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","    test_dir,\n","    image_size=img_size,\n","    batch_size=batch_size\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":331,"status":"ok","timestamp":1723384889811,"user":{"displayName":"Steven Tian","userId":"04559681777298250097"},"user_tz":240},"id":"Q5NZYdeF8C8R"},"outputs":[],"source":["# Define the CNN branch\n","def create_cnn_branch(input_shape=(224, 224, 3)):\n","    cnn_input = layers.Input(shape=input_shape)\n","    x = layers.Conv2D(32, kernel_size=(3, 3), activation='relu')(cnn_input)\n","    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n","    x = layers.Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n","    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n","    x = layers.Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n","    x = layers.GlobalAveragePooling2D()(x)\n","    cnn_output = layers.Dense(128, activation='relu')(x)\n","    return Model(cnn_input, cnn_output, name=\"CNN_Branch\")\n","\n","# Define ViT branch\n","def create_vit_branch(input_shape=(224, 224, 3), patch_size=16, num_layers=4, num_heads=8, projection_dim=128, mlp_dim=256, num_classes=7):\n","    vit_input = layers.Input(shape=input_shape)\n","\n","    #Extract patches\n","    patches = layers.Conv2D(filters=projection_dim, kernel_size=(patch_size, patch_size), strides=(patch_size, patch_size))(vit_input)\n","    patches = layers.Reshape((-1, projection_dim))(patches)\n","\n","    # Positional encoding\n","    patch_positions = tf.range(start=0, limit=patches.shape[1], delta=1)\n","    positional_encoding = layers.Embedding(input_dim=patches.shape[1], output_dim=projection_dim)(patch_positions)\n","    positional_encoding = tf.expand_dims(positional_encoding, axis=0)\n","    patches += positional_encoding\n","\n","    # Transformer layers\n","    for _ in range(num_layers):\n","        #Multi head Self Attention\n","        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim)(patches, patches)\n","        x = layers.Add()([attention_output, patches])\n","        x = layers.LayerNormalization()(x)\n","\n","        #MLP\n","        x = layers.Dense(mlp_dim, activation='relu')(x)\n","        x = layers.Dense(projection_dim)(x)\n","        patches = layers.Add()([x, patches])\n","        patches = layers.LayerNormalization()(patches)\n","\n","    vit_output = layers.GlobalAveragePooling1D()(patches)\n","    vit_output = layers.Dense(128, activation='relu')(vit_output)\n","\n","    return Model(vit_input, vit_output, name=\"ViT_Branch\")\n","\n","# Combine CNN and ViT\n","def create_hybrid_model(input_shape=(224, 224, 3), num_classes=7):\n","    cnn_branch = create_cnn_branch(input_shape=input_shape)\n","    vit_branch = create_vit_branch(input_shape=input_shape)\n","\n","    combined = layers.concatenate([cnn_branch.output, vit_branch.output])\n","    x = layers.Dense(256, activation='relu')(combined)\n","    x = layers.Dropout(0.5)(x)\n","    x = layers.Dense(128, activation='relu')(x)\n","    output = layers.Dense(num_classes, activation='softmax')(x)\n","\n","    hybrid_model = Model(inputs=[cnn_branch.input, vit_branch.input], outputs=output, name=\"Hybrid_Model\")\n","    return hybrid_model\n","\n","#define model\n","model = create_hybrid_model()\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","#model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"n2Oj-287-nIM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1692s\u001b[0m 2s/step - accuracy: 0.2110 - loss: 2.2946 - val_accuracy: 0.2489 - val_loss: 1.8065\n","Epoch 2/20\n","\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1470s\u001b[0m 2s/step - accuracy: 0.2359 - loss: 1.8191 - val_accuracy: 0.2513 - val_loss: 1.7996\n","Epoch 3/20\n","\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1444s\u001b[0m 2s/step - accuracy: 0.2450 - loss: 1.8028 - val_accuracy: 0.2492 - val_loss: 1.7922\n","Epoch 4/20\n","\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1412s\u001b[0m 2s/step - accuracy: 0.2481 - loss: 1.7952 - val_accuracy: 0.2492 - val_loss: 1.7891\n","Epoch 5/20\n","\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1426s\u001b[0m 2s/step - accuracy: 0.2502 - loss: 1.7913 - val_accuracy: 0.2396 - val_loss: 1.8134\n","Epoch 6/20\n","\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1479s\u001b[0m 2s/step - accuracy: 0.2547 - loss: 1.7869 - val_accuracy: 0.2534 - val_loss: 1.7790\n","Epoch 7/20\n","\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1453s\u001b[0m 2s/step - accuracy: 0.2575 - loss: 1.7814 - val_accuracy: 0.2494 - val_loss: 1.7974\n","Epoch 8/20\n","\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1431s\u001b[0m 2s/step - accuracy: 0.2600 - loss: 1.7791 - val_accuracy: 0.2480 - val_loss: 1.7928\n","Epoch 9/20\n","\u001b[1m459/718\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m7:39\u001b[0m 2s/step - accuracy: 0.2585 - loss: 1.7747"]}],"source":["def duplicate_input(image, label):\n","    return (image, image), label\n","\n","train_dataset = train_dataset.map(duplicate_input)\n","val_dataset = val_dataset.map(duplicate_input)\n","test_dataset = test_dataset.map(duplicate_input)\n","\n","epochs = 20\n","batch_size = 32\n","\n","history = model.fit(\n","    train_dataset,\n","    epochs=20,\n","    validation_data=val_dataset\n",")\n","\n","val_loss, val_accuracy = model.evaluate([val_dataset, val_dataset])\n","print(f'Validation Loss: {val_loss}')\n","print(f'Validation Accuracy: {val_accuracy}')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}