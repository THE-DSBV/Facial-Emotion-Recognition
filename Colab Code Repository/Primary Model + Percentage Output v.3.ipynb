{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-DRK_o-Syau4dt9mnsYL5W8XqSHOG_h-","timestamp":1723342223303},{"file_id":"1cnvfhF30gujaM40ZBMgY-q5RlDpHI9NB","timestamp":1720057306746}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFRYIdLHc1uv","executionInfo":{"status":"ok","timestamp":1723347103789,"user_tz":240,"elapsed":32919,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}},"outputId":"62dd9abb-c8f2-4558-bfe5-4c55a6d76d2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from zipfile import ZipFile\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torch.utils.data import Dataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data import Subset\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import shutil\n","from pathlib import Path\n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","use_cuda = True\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Define class EmotionDataset\n","\n","class EmotionDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","        # Map each emotion to indices from 0 ~ 6\n","        self.classes = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n","        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n","        self.images = []\n","        self.labels = []\n","\n","        for label in self.classes:\n","            class_dir = os.path.join(self.root_dir, label)\n","            for img_name in os.listdir(class_dir):\n","                img_path = os.path.join(class_dir, img_name)\n","                self.images.append(img_path)\n","                self.labels.append(label)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.images[idx]\n","        image = Image.open(img_path)\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Map label to index\n","        label_idx = self.class_to_idx[label]\n","        return image, label_idx"],"metadata":{"id":"aMHx3PahRdyv","executionInfo":{"status":"ok","timestamp":1723347117426,"user_tz":240,"elapsed":150,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Get indicies of the targeted classes\n","def get_relevant_indices(dataset, classes, target_classes):\n","    # dataset = [(img1, 0), (img2, 1), (img3, 2), (img4, 0), (img5, 1)]\n","\n","    \"\"\" Return the indices for datapoints in the dataset that belongs to the\n","    desired target classes, a subset of all possible classes.\n","\n","    Args:\n","        dataset: Dataset object\n","        classes: A list of strings denoting the name of each class\n","        target_classes: A list of strings denoting the name of desired classes\n","                        Should be a subset of the 'classes'\n","    Returns:\n","        indices: list of indices that have labels corresponding to one of the\n","                 target classes\n","    \"\"\"\n","    indices = []\n","    for i in range(len(dataset)):\n","        # Check if the label is in the target classes\n","        label_index = dataset[i][1] # ex: 3\n","        label_class = classes[label_index] # ex: 'A'\n","        if label_class in target_classes:\n","            indices.append(i)\n","\n","    # if target class being class0 & class1, then indices = [0, 1, 3, 4]\n","    return indices"],"metadata":{"id":"S2WR1OUdSWEw","executionInfo":{"status":"ok","timestamp":1723347283919,"user_tz":240,"elapsed":173,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Load and split data\n","def get_data_loader(data_directory, target_classes, batch_size):\n","    \"\"\" Loads images of facial emotions, splits the data into training, validation\n","    and testing datasets. Returns data loaders for the three preprocessed datasets.\n","\n","    Args:\n","        target_classes: A list of strings denoting the name of the desired\n","                        classes. Should be a subset of the argument 'classes'\n","        batch_size: A int representing the number of samples per batch\n","\n","    Returns:\n","        train_loader: iterable training dataset organized according to batch size\n","        val_loader: iterable validation dataset organized according to batch size\n","        test_loader: iterable testing dataset organized according to batch size\n","        classes: A list of strings denoting the name of each class\n","    \"\"\"\n","    # Letters for classification\n","    classes = ('angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised')\n","\n","    ########################################################################\n","    # The output of torchvision datasets are PILImage images of range [0, 1].\n","    # We transform them to Tensors of normalized range [-1, 1].\n","    transform = transforms.Compose(\n","         [transforms.Grayscale(1),\n","          transforms.Resize((48, 48)),\n","         transforms.ToTensor(),\n","         transforms.Normalize((0.5), (0.5))])\n","\n","    ########################################################################\n","    # Load Gesture training data using the defined gesture dataset class above\n","    dataset = EmotionDataset(root_dir=data_directory, transform=transform)\n","\n","    # Get the list of indices to sample from\n","    relevant_indices = get_relevant_indices(dataset, classes, target_classes)\n","\n","    # Split into 60% train, 20% validation and 20% testing (since folder contains all data available)\n","    np.random.seed(1000) # Fixed numpy random seed for reproducible shuffling\n","    np.random.shuffle(relevant_indices)\n","\n","    total_size = len(relevant_indices)\n","\n","    train_split = int(total_size * 0.7)  # 70% for training\n","    val_split = int(total_size * 0.15)    # 15% for validation\n","    test_split = total_size - train_split - val_split  # Remaining 15% for testing\n","\n","    # split into training and validation indices\n","    relevant_train_indices = relevant_indices[:train_split]\n","    relevant_val_indices = relevant_indices[train_split:train_split + val_split]\n","    relevant_test_indices = relevant_indices[train_split + val_split:]\n","\n","    ########################################################################\n","    train_sampler = SubsetRandomSampler(relevant_train_indices)\n","    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                               num_workers=1, sampler=train_sampler)\n","\n","    val_sampler = SubsetRandomSampler(relevant_val_indices)\n","    val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                              num_workers=1, sampler=val_sampler)\n","\n","    test_sampler = SubsetRandomSampler(relevant_test_indices)\n","    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                              num_workers=1, sampler=test_sampler)\n","\n","    return train_loader, val_loader, test_loader, classes"],"metadata":{"id":"wmtyh30aRw-T","executionInfo":{"status":"ok","timestamp":1723347285835,"user_tz":240,"elapsed":401,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Load the data in\n","train_loader, val_loader, test_loader, classes = get_data_loader(\n","    data_directory = '/content/drive/My Drive/APS360 Tut & Labs/Processed_Dataset',\n","    target_classes=['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised'],\n","    batch_size=1) # 1 image per batch\n","\n","print(\"Dataset loaded successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CuStMfzTRxi2","executionInfo":{"status":"ok","timestamp":1723347438438,"user_tz":240,"elapsed":150344,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}},"outputId":"27fc789a-9d9e-43c5-cd29-b55c6233c111"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset loaded successfully.\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class EmotionRecognition(nn.Module):\n","    def __init__(self, num_classes=6):\n","        super(EmotionRecognition, self).__init__()\n","\n","        # Convolutional layers\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","\n","        # Pooling layer\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(128 * 6 * 6, 512)\n","        self.fc2 = nn.Linear(512, num_classes)\n","\n","        # Dropout layer\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","\n","        x = x.view(-1, 128 * 6 * 6)  # Flatten the tensor\n","\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        return x\n","\n","\n","model = EmotionRecognition(num_classes=6)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"dihQeS1JBwHU","executionInfo":{"status":"ok","timestamp":1723348749022,"user_tz":240,"elapsed":252,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#Get model name for checkpointing\n","def get_model_name(name, batch_size, learning_rate, epoch):\n","   path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size,\n","                                                  learning_rate, epoch)\n","   return path"],"metadata":{"id":"zO4OChzywuCN","executionInfo":{"status":"ok","timestamp":1723348750911,"user_tz":240,"elapsed":180,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def get_accuracy(model, data_loader):\n","    #initialize counters\n","    correct = 0\n","    total = 0\n","    for imgs, labels in data_loader:\n","        #Enable GPU if available\n","        if use_cuda and torch.cuda.is_available():\n","          imgs = imgs.cuda()\n","          labels = labels.cuda()\n","\n","        output = model(imgs)\n","        #select index with maximum prediction score\n","        pred = output.max(1, keepdim=True)[1]\n","        correct += pred.eq(labels.view_as(pred)).sum().item()\n","        total += imgs.shape[0]\n","    #Return accuracy\n","    return correct / total"],"metadata":{"id":"bm7i5jBvwu6N","executionInfo":{"status":"ok","timestamp":1723348754514,"user_tz":240,"elapsed":184,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def plot_training_curve(path, num_epochs, train_acc, val_acc):\n","    plt.title(\"Training Curve\")\n","    n = len(num_epochs) # number of epochs\n","    plt.plot(range(1,n+1), train_acc, label=\"Train\")\n","    plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Error\")\n","    plt.legend(loc='best')\n","    plt.show()"],"metadata":{"id":"HdZrhasIFCr6","executionInfo":{"status":"ok","timestamp":1723348757032,"user_tz":240,"elapsed":270,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, val_loader, batch_size=64, num_epochs=30,\n","          learn_rate=0.001, model_name='EmotionRecognition'):\n","    #Set seed\n","    torch.manual_seed(1700)\n","\n","    #Set loss function and optimizer\n","    #I chose to use cross entropy loss for the loss function because\n","    #we have been taught that this is generally the best performing\n","    #loss function for multiclass classification problems.\n","    #I chose to use Adam as the optimizer because according to my\n","    #research, Adam generally provides very good results without\n","    #significant fine tuning.\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n","    emotion_tracker = np.zeros(7)\n","    percentage_tracker = np.zeros(7)\n","    emotions = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n","\n","    #populate accuracy arrays with zeroes\n","    train_acc = np.zeros(num_epochs)\n","    val_acc = np.zeros(num_epochs)\n","\n","    #initialize iteration counter\n","    n = 0\n","    for epoch in range(num_epochs):\n","        for images, labels in iter(train_loader):\n","            #Enable GPU if available\n","            if use_cuda and torch.cuda.is_available():\n","              images = images.cuda()\n","              labels = labels.cuda()\n","\n","            #perform forwards pass\n","            out = model(images)\n","            #Compute loss using Cross Entropy loss function\n","            loss = criterion(out, labels)\n","            tracker_index = torch.argmax(out)\n","            emotion_tracker[tracker_index] += 1\n","\n","            #backward pass to calculate and update parameters.\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            #increment iteration counter\n","            n += 1\n","\n","        # track training and validation accuracy\n","        train_acc[epoch] = get_accuracy(model, train_loader)\n","        val_acc[epoch] = get_accuracy(model, val_loader)\n","\n","        #output accuracy\n","        print((\"Epoch: {} | Training acc: {} |\" + \"Validation acc: {}\"\n","        ).format(epoch, train_acc[epoch], val_acc[epoch]))\n","\n","        #checkpoint model\n","        model_path = get_model_name(model_name, batch_size, learn_rate,\n","                                    epoch)\n","        torch.save(model.state_dict(), model_path)\n","\n","    epochs = np.arange(1, num_epochs + 1)\n","\n","\n","    plot_training_curve(model_path, epochs, train_acc, val_acc)\n","\n","    # output percentages:\n","    total = torch.sum(emotion_tracker)\n","    for e in range(len(emotion_tracker)):\n","      percentage_tracker[e] = float(emotion_tracker[e]/total)\n","      print(emotions[e], \":\", f\"{percentage_tracker[e]:.2%}\")\n","\n","    print(\"Prediction:\", emotions[np.argmax(percentage_tracker)], \"with\", f\"{(percentage_tracker[np.argmax(percentage_tracker)]):.2%}\", \"confidence.\")"],"metadata":{"id":"Cu6bifKYGmUC","executionInfo":{"status":"ok","timestamp":1723350569192,"user_tz":240,"elapsed":173,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# test\n","import torch\n","import numpy as np\n","\n","emotion_tracker = torch.tensor([6., 2., 5., 5., 3., 1., 0.])\n","emotions = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n","percentage_tracker = np.zeros(7)\n","\n","total = torch.sum(emotion_tracker)\n","for e in range(len(emotion_tracker)):\n","      percentage_tracker[e] = float(emotion_tracker[e]/total)\n","      print(emotions[e], \":\", f\"{percentage_tracker[e]:.2%}\")\n","\n","print(\"Prediction:\", emotions[np.argmax(percentage_tracker)], \"with\", f\"{(percentage_tracker[np.argmax(percentage_tracker)]):.2%}\", \"confidence.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76yh_7x7PMKb","executionInfo":{"status":"ok","timestamp":1723410887632,"user_tz":240,"elapsed":171,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}},"outputId":"6b2c8a1e-3ac5-4f0d-d4f7-0d8fe1dffa70"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["angry : 27.27%\n","disgusted : 9.09%\n","fearful : 22.73%\n","happy : 22.73%\n","neutral : 13.64%\n","sad : 4.55%\n","surprised : 0.00%\n","Prediction: angry with 27.27% confidence.\n"]}]},{"cell_type":"code","source":["if use_cuda and torch.cuda.is_available():\n","  model.cuda()\n","\n","train(model, train_loader, val_loader, num_epochs = 30)\n"],"metadata":{"id":"vG56Oi4gG0y9","colab":{"base_uri":"https://localhost:8080/","height":740},"executionInfo":{"status":"error","timestamp":1723350574412,"user_tz":240,"elapsed":1966,"user":{"displayName":"Remi Li","userId":"07213625586389796104"}},"outputId":"b4eff026-86d0-4751-b0a2-0f79cfc3a441"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[1. 0. 0. 0. 0. 0. 0.]\n","[1. 0. 0. 1. 0. 0. 0.]\n","[1. 0. 1. 1. 0. 0. 0.]\n","[1. 0. 1. 1. 1. 0. 0.]\n","[1. 0. 1. 2. 1. 0. 0.]\n","[1. 1. 1. 2. 1. 0. 0.]\n","[1. 1. 1. 3. 1. 0. 0.]\n","[2. 1. 1. 3. 1. 0. 0.]\n","[3. 1. 1. 3. 1. 0. 0.]\n","[3. 1. 2. 3. 1. 0. 0.]\n","[3. 2. 2. 3. 1. 0. 0.]\n","[4. 2. 2. 3. 1. 0. 0.]\n","[4. 2. 2. 4. 1. 0. 0.]\n","[4. 2. 2. 4. 2. 0. 0.]\n","[4. 2. 2. 4. 2. 1. 0.]\n","[5. 2. 2. 4. 2. 1. 0.]\n","[5. 2. 2. 4. 3. 1. 0.]\n","[5. 2. 3. 4. 3. 1. 0.]\n","[5. 2. 3. 5. 3. 1. 0.]\n","[6. 2. 3. 5. 3. 1. 0.]\n","[6. 2. 4. 5. 3. 1. 0.]\n","[6. 2. 5. 5. 3. 1. 0.]\n"]},{"output_type":"error","ename":"IndexError","evalue":"Target 6 is out of bounds.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-e39655bdbb6d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-c4797f99158c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, batch_size, num_epochs, learn_rate, model_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#Compute loss using Cross Entropy loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mtracker_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0memotion_tracker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtracker_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1186\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                                label_smoothing=self.label_smoothing)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Target 6 is out of bounds."]}]}]}