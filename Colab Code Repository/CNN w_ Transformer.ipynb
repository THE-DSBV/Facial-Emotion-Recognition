{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMPsT8+z+HhmwZHeDQTmwq2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5m_luDphgN5J"},"outputs":[],"source":["from zipfile import ZipFile\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data import Subset\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import shutil\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling1D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","\n","from tensorflow.keras import mixed_precision\n","from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Add\n"]},{"cell_type":"code","source":["# PatchEmbedding Class\n","class PatchEmbedding(layers.Layer):\n","    def __init__(self, patch_size, projection_dim):\n","        super(PatchEmbedding, self).__init__()\n","        self.patch_size = patch_size\n","        self.projection_dim = projection_dim\n","        self.projection = tf.keras.layers.Dense(projection_dim)\n","\n","    def call(self, cnn_features):\n","        batch_size = tf.shape(cnn_features)[0]\n","        patch_size = self.patch_size\n","\n","        # Reshape the CNN features to patches\n","        patches = tf.image.extract_patches(\n","            images=cnn_features,\n","            sizes=[1, patch_size, patch_size, 1],\n","            strides=[1, patch_size, patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding='VALID'\n","        )\n","\n","        patch_dim = patches.shape[-1]\n","        num_patches = patches.shape[1] * patches.shape[2]  # Calculate the number of patches\n","\n","        patches = tf.reshape(patches, (batch_size, num_patches, patch_dim))\n","        embeddings = self.projection(patches)\n","\n","        return embeddings\n","\n","# TransformerEncoderLayer Class\n","class TransformerEncoderLayer(layers.Layer):\n","    def __init__(self, num_heads, embedding_dim, mlp_dim, dropout_rate=0.1):\n","        super(TransformerEncoderLayer, self).__init__()\n","        self.layer_norm1 = layers.LayerNormalization()\n","        self.multi_head_attention = layers.MultiHeadAttention(num_heads, embedding_dim)\n","        self.add1 = layers.Add()\n","        self.layer_norm2 = layers.LayerNormalization()\n","        self.mlp = models.Sequential([\n","            layers.Dense(mlp_dim, activation='relu'),\n","            layers.Dense(embedding_dim)\n","        ])\n","        self.add2 = layers.Add()\n","        self.dropout = layers.Dropout(dropout_rate)\n","\n","    def call(self, x):\n","        x1 = self.layer_norm1(x)\n","        attention_output = self.multi_head_attention(x1, x1)\n","        x2 = self.add1([x, attention_output])\n","        x3 = self.layer_norm2(x2)\n","        x3 = self.mlp(x3)\n","        x4 = self.add2([x2, x3])\n","        return self.dropout(x4)\n","\n","# CNN_EmotionClassifier Class (PyTorch)\n","class CNN_EmotionClassifier(nn.Module):\n","    def __init__(self, num_classes=7):\n","        super(CNN_EmotionClassifier, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","\n","        self.res_conv1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n","        self.res_bn1 = nn.BatchNorm2d(128)\n","        self.res_conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n","        self.res_bn2 = nn.BatchNorm2d(128)\n","\n","\n","        self.fc1 = nn.Linear(128 * 6 * 6, 512)\n","        self.fc2 = nn.Linear(512, num_classes)\n","\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = self.adaptive_pool(F.relu(self.bn1(self.conv1(x))))\n","        x = self.adaptive_pool(F.relu(self.bn2(self.conv2(x))))\n","        x = self.adaptive_pool(F.relu(self.bn3(self.conv3(x))))\n","\n","        residual = x\n","        x = F.relu(self.res_bn1(self.res_conv1(x)))\n","        x = self.res_bn2(self.res_conv2(x))\n","        x += residual\n","        x = F.relu(x)\n","\n","        x = self.adaptive_pool(x)\n","        x = x.view(-1, 128 * 6 * 6)\n","\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        return x\n","\n","# CNN_Transformer_Model Class\n","class CNN_Transformer_Model(tf.keras.Model):\n","    def __init__(self, cnn, image_size, patch_size, num_heads, projection_dim, mlp_dim, num_classes, num_transformer_layers, dropout_rate=0.1):\n","        super(CNN_Transformer_Model, self).__init__()\n","        self.cnn = cnn\n","        self.patch_size = patch_size\n","        self.num_patches = (image_size // patch_size) ** 2\n","        self.patch_embedding = PatchEmbedding(patch_size, projection_dim)\n","        self.transformer_layers = [TransformerEncoderLayer(num_heads, projection_dim, mlp_dim, dropout_rate) for _ in range(num_transformer_layers)]\n","        self.global_pool = layers.GlobalAveragePooling1D()\n","        self.dense = layers.Dense(num_classes, activation='softmax')\n","\n","    def call(self, inputs):\n","        cnn_features = self.cnn(inputs)  # Pass inputs through CNN\n","        patches = self.patch_embedding(cnn_features)\n","        x = patches\n","        for layer in self.transformer_layers:\n","            x = layer(x)\n","        x = self.global_pool(x)\n","        output = self.dense(x)\n","        return output\n","\n","    '''cnn_features = self.cnn(inputs)  # Pass inputs through CNN\n","\n","        patches = tf.image.extract_patches(\n","            images=cnn_features,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding='VALID'\n","        )\n","\n","        batch_size = tf.shape(patches)[0]\n","        patch_dim = patches.shape[-1]\n","        patches = tf.reshape(patches, (batch_size, self.num_patches, patch_dim))\n","\n","        x = self.patch_embedding(patches)\n","        for layer in self.transformer_layers:\n","            x = layer(x)\n","\n","        x = self.global_pool(x)\n","        output = self.dense(x)\n","        return output'''\n","\n","# Instantiate the CNN model and convert to a Keras model\n","\n","#UNCOMMENT the .to('cuda') part of this line if you're using GPU\n","cnn_model = CNN_EmotionClassifier(num_classes=7)#.to('cuda')\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n","  # Set the PyTorch model to evaluation mode\n","cnn_model.eval()\n","\n","cnn_model_tf = tf.keras.models.Sequential([\n","    layers.Conv2D(5, 5, activation='relu', input_shape=(224, 224, 3)),\n","    layers.MaxPooling2D(pool_size=(2, 2)),\n","    layers.Conv2D(10, 5, activation='relu'),\n","    layers.MaxPooling2D(pool_size=(2, 2))\n","])\n","'''\n","cnn_model_tf = tf.keras.models.Sequential([\n","    layers.Conv2D(5, 5, activation='relu', input_shape=(224, 224, 3)),\n","    layers.MaxPooling2D(pool_size=(2, 2)),\n","    layers.Conv2D(10, 5, activation='relu'),\n","    layers.MaxPooling2D(pool_size=(2, 2)),\n","    layers.Flatten(),\n","    layers.Dense(32, activation='relu'),\n","])'''\n","\n","# Instantiate the CNN-Transformer model\n","\n","#Hyperparameters\n","image_size = 224\n","patch_size = 32\n","num_heads = 8\n","projection_dim = 128\n","mlp_dim = 256\n","num_transformer_layers = 4\n","dropout_rate = 0.1\n","num_classes = 7\n","\n","cnn_transformer_model = CNN_Transformer_Model(\n","    cnn=cnn_model_tf,\n","    image_size=image_size,\n","    patch_size=patch_size,\n","    num_heads=num_heads,\n","    projection_dim=projection_dim,\n","    mlp_dim=mlp_dim,\n","    num_classes=num_classes,\n","    num_transformer_layers=num_transformer_layers,\n","    dropout_rate=dropout_rate\n",")\n","\n","# Compile the model\n","cnn_transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","dataset_path = '/content/gdrive/MyDrive/APS360 Project/Data/train'\n","\n","# Load and prepare the dataset\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    dataset_path,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=123,\n","    image_size=(224,224),\n","    batch_size=32,\n","    label_mode='categorical'\n",").cache().prefetch(tf.data.experimental.AUTOTUNE)\n","\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    dataset_path,\n","    validation_split=0.2,\n","    subset=\"validation\",\n","    seed=123,\n","    image_size=(224,224),\n","    batch_size=32,\n","    label_mode='categorical'\n",").cache().prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# Train the model\n","history = cnn_transformer_model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=10,\n","    callbacks=[\n","        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n","        tf.keras.callbacks.ModelCheckpoint('cnn_transformer_model.keras', save_best_only=True),\n","        tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)\n","    ]\n",")\n","\n","# Evaluate the model\n","final_val_loss, final_val_accuracy = cnn_transformer_model.evaluate(val_ds)\n","print(f\"Final Validation Loss: {final_val_loss:.4f}, Final Validation Accuracy: {final_val_accuracy:.4f}\")\n","\n","# Save the model\n","cnn_transformer_model.save('cnn_transformer_model.keras')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6XTTIhcIvrBV","outputId":"a244d3dc-6705-4b81-cde7-0c2a42559aea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","Found 28710 files belonging to 7 classes.\n","Using 22968 files for training.\n","Found 28710 files belonging to 7 classes.\n","Using 5742 files for validation.\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m 48/718\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m48:07\u001b[0m 4s/step - accuracy: 0.1706 - loss: 330.6751"]}]}]}